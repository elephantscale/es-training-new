# Databricks on AWS

(C) Copyright Elephant Scale

## Overview

![](../assets/images/logos/spark-logo-1-small.png)

Databricks is a unified analytics platform that allows for large-scale data processing and machine learning. When you deploy Databricks on AWS (Amazon Web Services), you leverage AWS's powerful cloud infrastructure to handle big data analytics and processing tasks.
This hand-on course in Apache Spark is geared for technical business professional who wish to solve real-world data problems using Apache Spark.

This class is taught with Python language and using Jupyter environment. 

## What You Will Learn

* Spark ecosystem
* PySpark and its place in Spark ecosystem
* New features in Spark3
* How to use Databricks Spark on AWS
* Spark Shell
* Spark Data structures (RDD / Dataframe / Dataset)
* Spark SQL
* Modern data formats and Spark
* Spark API
* Spark Streaming

## Duration

Three Days

## Audience

Developers, Data Analysts, Business Analysts

## Skill level

Introductory to Intermediate

## Prerequisites

* Basic knowledge of Python language and Jupyter notebooks is preferred but not mandatory
Even if you haven't done any Python programming, Python is such an easy language to learn quickly.  We will provide Python resources.

## Lab Environment

* Labs will be provided online

## Students will need the following

* A reasonably modern laptop with unrestricted connection to the Internet.  Laptops with overly restrictive VPNs or firewalls may not work properly
* Chrome browser

## Detailed Course Outline

Here's an outline that provides an overview of using Databricks on AWS:

1. **Introduction**
    - What is Databricks?
    - Overview of AWS services.
    - Benefits of deploying Databricks on AWS.

2. **Setting Up Databricks on AWS**
    - AWS prerequisites:
        - Setting up an AWS account.
        - Understanding of IAM (Identity and Access Management).
    - Databricks workspace setup:
        - Creating a Databricks workspace.
        - Configuration options.
    - Network and Security:
        - VPC Peering and setup.
        - Security groups and firewalls.

3. **Integrating with AWS Services**
    - Amazon S3:
        - Setting up S3 buckets.
        - Integrating Databricks with S3 for storage.
        - Data access and management.
    - Amazon RDS & Redshift:
        - Setting up and integrating with relational databases and data warehouses.
    - Amazon EC2:
        - Understanding cluster management.
        - Optimizing compute resources.
    - AWS IAM:
        - Role-based access controls.
        - Databricks token management.

4. **Data Processing with Databricks**
    - Apache Spark on Databricks:
        - Overview of Spark's capabilities.
        - Using Spark for data processing, ETL tasks, and analytics.
    - Delta Lake:
        - Benefits of Delta Lake on Databricks.
        - Optimizing performance and reliability.
    - Streaming Analytics:
        - Integrating with AWS Kinesis.
        - Real-time data processing.

5. **Machine Learning with Databricks**
    - MLflow:
        - Experiment tracking, model registry, and deployment.
    - Integration with AWS SageMaker:
        - Deploying models, real-time prediction.
    - Distributed Machine Learning:
        - Training models at scale with Spark MLlib.

6. **Monitoring and Optimization**
    - Databricks cluster management:
        - Autoscaling, spot instances, and optimizing costs.
    - Monitoring with AWS CloudWatch.
    - Logging and auditing.

7. **Cost Management**
    - Understanding Databricks and AWS billing.
    - Optimizing costs using reserved instances, spot pricing.
    - Setting up budgets and alerts.

8. **Best Practices**
    - Data security and compliance.
    - Performance tuning.
    - Collaborative features of Databricks.

9. **Conclusion**
    - Reviewing the advantages of Databricks on AWS.
    - Future trends and evolutions.
    - Resources for deeper learning.

10. ** References **
    - https://www.databricks.com/product/aws