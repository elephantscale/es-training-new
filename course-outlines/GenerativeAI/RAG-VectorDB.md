# Generative AI with RAG and VectorDB

(C) Copyright Elephant Scale
April 14, 2024

## Course Description

* Generative AI with Large Language Models (LLM) open ways to building smart application as never before. 
* The most popular architecture for that is Retrieval-Augmented Generation (RAG.) RAG systems are built with semantic search. 
* In this course, the students learn how build the RAG systems. 
  * For semantic component we use VectorDB from DataStax. 
  * For the Generative AI, we have a choice of LLMs, such as ChatGPT or LLama.
  * For the implementation, we teach the best cloud and cloud architecture for your project.

## After the course, you will be able to do the following tasks

* Talk to an LLM in a correct way.
* Script talking to LLM for a programmatic implementation.
* Organize your private documents for the implementation and break them into meaningful fragments for storing in the semantic search engine (VectorDB.)
* Structure the flow of conversation with LLM about your private documents.
* Implement production, testing, and continuous improvements

## Audience
* Developers, data scientists, team leads, project managers

## Skill Level

* Intermediate

## Duration
* Two days

## Prerequisites
* General familiarity with machine learning


## Format
* Lectures and hands on labs. (50% - 50%)


## Lab environment
* Zero Install: There is no need to install software on students' machines!
* A lab environment in the cloud will be provided for students.

### Students will need the following
* A reasonably modern laptop with unrestricted connection to the Internet. Laptops with overly restrictive VPNs or firewalls may not work properly.
    * A checklist to verify connectivity will be provided
* Chrome browser

## Detailed outline

### Introduction to Deep Learning
- Understanding Deep Learning use cases
- Overview of Neural Networks: NN, CNN, RNN.

### Prompt engineering for LLM's
* Best practices
* Practical advice

### HuggingFace offering

* Transformers library
* Models 
* Putting it all together


### Fine tuning a pretrained model
* Processing the data
* Fine-tuning a model with the Trainer API or Keras
* A full training


### Main NLP tasks
* Token classification
* Fine-tuning a masked language model
* Translation
* Summarization
* Training a causal language model from scratch
* Question answering
* Mastering NLP

### Open LLM
* Overview of LLMs available
* Comparison of capabilities
* Evaluating and fine-tuning an LLM
* Alpaca from Stanford
* LLama from Facebook
* Dolly from Databricks
* Nomic
* Vicuna

```text

```
