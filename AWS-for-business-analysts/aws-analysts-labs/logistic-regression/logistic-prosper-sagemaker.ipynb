{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "## Overview\n",
    "In this lab, we will be applying logistic regression on \"prosper-loan\" data which is an individual loan data set provided by the P2P lending company, Prosper.\n",
    "Our goal is to predict whether or not an individual can get a loan based on his/her credentials.\n",
    "\n",
    "\n",
    "## Downloading S3 Data into Jupyter Notebook\n",
    "\n",
    "First, we need to upload the data required to run the lab into an S3 bucket.\n",
    "\n",
    "Next, we need to create another empty S3 bucket starting with the name “Sagemaker”. This bucket is for storing the model, results, etc., after running the algorithm.\n",
    "\n",
    "Download the data file <give path here> from S3 - you should see a message like this:\n",
    "\n",
    "<img src=\"../assets/images/logistic-regression/pic1.jpg\" style=\"width:50%\"/>\n",
    "\n",
    "---\n",
    "\n",
    "## Preparing the data for Training\n",
    "\n",
    "* The data has more than 50 columns. We will simplify it by just taking 6 columns. We will use the following columns: 'LoanStatus','BorrowerRate','EmploymentStatus', 'CreditScore', 'StatedMonthlyIncome','IncomeVerifiable' alone.\n",
    "\n",
    "<img src=\"../assets/images/logistic-regression/pic2.jpg\" style=\"width:50%\"/>\n",
    "\n",
    "* Convert the categorical values into numeric by using one-hot encoding.\n",
    "Pandas library provides in-built functions for encoding the data. Let's do this for the \"Employment Status\" column.\n",
    "\n",
    "<img src=\"../assets/images/logistic-regression/pic3.jpg\" style=\"width:50%\"/>\n",
    "\n",
    "* Convert \"IncomeVerifiable\" column into integer values. We will assign 1 for True and 0 for False.\n",
    "\n",
    "<img src=\"../assets/images/logisitic-regression/pic4.jpg\" style=\"width:50%\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "Now, all the columns are float variables. So, we can proceed for training the data.\n",
    "\n",
    "---\n",
    "\n",
    "Before, giving input to the algorithm, we divide the data into features and labels. Labels is the prediction column. In this case, it is the \"LoanStatus\" column.\n",
    "Features are all the columns except the \"LoanStatus\" column.\n",
    "\n",
    "Next, we create a bucket in S3 that begins with the letters \"sagemaker\". Then Amazon will create the subfolders, which in needs, which in this case are sagemaker/grades and others. It is important that we create the S3 buckets in the same Amazon region as our notebook. Otherwise Amazon will throw an error saying it cannot find the data. See the note below on that.\n",
    "Copy this text into a notebook cell and then run it.\n",
    "\n",
    "<img src=\"../assets/images/logistic-regression/pic5.jpg\" style=\"width:50%\"/>\n",
    "\n",
    "We will get an output displaying the location where the training data and artifacts will be stored.\n",
    "\n",
    "Next, we create a docker container in the same region where our notebook is created. In our case, it is \" us-east-1\"\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Setting up the Logistic Regression Estimator\n",
    "\n",
    "Now we begin to set up the Estimator. Amazon will not let you use any of their smaller (i.e. less expensive) images, so here we use a virtual machine of size ml.p2.xlarge.\n",
    "\n",
    "\n",
    "\n",
    "Now we provide hyperparameters. There are many, like which loss function to use. Here we put only the most important ones:\n",
    "\n",
    "feature_dim—is the number of columns in our feature array. In this case it is 12.\n",
    "mini_batch_size—is the number of batches into which to split the data. This number should be smaller than the number of records in our training set. We only have 20000 records, so we take 2000 as batch size.\n",
    "predictor_type—we use binary_classifier, which means logistic regression.\n",
    "\n",
    "When you run the fit() method Amazon will kick off this job. This will take several minutes to run.\n",
    "\n",
    "<img src=\"../assets/images/logistic-regression/pic6.jpg\" style=\"width:50%\"/>\n",
    "\n",
    "\n",
    "a.x(a1, b1)\n",
    "\n",
    "# Calculate precision by calling x and passing it ...\n",
    "a.x (?? , ??)\n",
    "---\n",
    "\n",
    "## Deploying the Model\n",
    "\n",
    "When the training model is done, deploy it to an endpoint. Remember that Amazon is charging you money now. So when you get done delete your endpoints unless you want to be charged.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Validating the Dataset\n",
    "\n",
    "Now we run the prediction. We just take some 10 data points and run the prediction.  \n",
    "\n",
    "<img src=\"../assets/images/logistic-regression/pic1.jpg\" style=\"width:50%\"/>\n",
    "\n",
    "That's the end of Logistic Regession session. You can try with different data and observe the results. You can also find the accuracy of the model using some techniques like Pseudo-R², precision, recall etc.,\n",
    "\n",
    "## ToDo\n",
    "\n",
    "To find the accuracy of the model,\n",
    "Calculate,\n",
    "1. Precision\n",
    "2. Recall\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Get Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading S3 Data into Jupyter Notebook\n",
    "\n",
    "# Firstly, we will download the data file from amazon S3 into our directory, \n",
    "# if we are able to download it, we will get a message like below,\n",
    "\n",
    "# !wget 'http://datakmeans.s3.amazonaws.com/prosper-loan-data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : Modify these to your own bucket\n",
    "bucket = \"elephantscale-sagemaker\"\n",
    "prefix = \"elephantscale-sagemaker/prosper\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "location = 'http://datakmeans.s3.amazonaws.com/prosper-loan-data.csv'\n",
    "# location = 'https://s3.amazonaws.com/elephantscale-public/data/prosper-loan/prosper-loan-data-sample.csv'\n",
    "prosper_data = pd.read_csv(location, header=0)\n",
    "prosper_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data for Training\n",
    "\n",
    "# The data has more than 50 columns. We will make it simple by just taking 6 columns. \n",
    "# We will consider 'LoanStatus','BorrowerRate','EmploymentStatus', 'CreditScore', 'StatedMonthlyIncome','IncomeVerifiable' alone.\n",
    "\n",
    "prosper_final = prosper_data[['LoanStatus','BorrowerRate','EmploymentStatus', 'CreditScore', 'StatedMonthlyIncome','IncomeVerifiable']]\n",
    "prosper_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the execution role for the notebook instance. \n",
    "# This is the IAM role that you created when you created your notebook instance. \n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next step is to convert the categoriacal values into one-hot encoding. \n",
    "# Pandas library provides in-built functions for converting the columns into one-hot encoding. \n",
    "# In our data, we will be converting \"Employment Status\" column.\n",
    "\n",
    "\n",
    "EmpStatus_onehot = pd.get_dummies(prosper_final['EmploymentStatus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will remove the EmploymentStatus Column\n",
    "prosper_final = prosper_final.drop('EmploymentStatus',axis = 1)\n",
    "prosper_final = prosper_final.join(EmpStatus_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prosper_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(prosper_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we will convert \"IncomeVerifiable\" column into integer values. \n",
    "# We will assign 1 for True and 0 for False. \n",
    "\n",
    "IncomeVerifiable_int = []\n",
    "for i in range(len(prosper_final)):\n",
    "    IncomeVerifiable_int.append(int(prosper_final['IncomeVerifiable'][i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prosper_final['IncomeVerifiable_int'] = IncomeVerifiable_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prosper_final1 = prosper_final.drop('IncomeVerifiable',axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prosper_final1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#We are taking 20000 rows for training and convert into np.array\n",
    "\n",
    "import numpy as np\n",
    "a = np.array(prosper_final1[:20000]).astype('float32')\n",
    "\n",
    "# Now, all the columns are float variables. So, we can proceed for training the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we divide the data into features and labels. \n",
    "# Labels is the prediction column. In this case, it is the \"LoanStatus\" column.\n",
    "# Features are all the columns except the \"LoanStatus\" column.\n",
    "\n",
    "features = a[:,1:]\n",
    "labels = a[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, create a bucket in S3 that begins with the letters \"sagemaker\". \n",
    "# SageMaker will create the subfolders it needs automatically. \n",
    "# It is important that you create the S3 buckets in the same Amazon region as the notebook. \n",
    "# Otherwise you will get an error saying it cannot find data. See the note below on that.\n",
    "\n",
    "#You should see an output displaying the location where the training data and artifacts will be stored. \n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "import io\n",
    "import sagemaker.amazon.common as smac\n",
    "import os\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, a, labels)\n",
    "buf.seek(0)\n",
    "\n",
    "key = 'linearlearner'\n",
    "boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train', key)).upload_fileobj(buf)\n",
    "s3_train_data = 's3://{}/{}/train/{}'.format(bucket, prefix, key)\n",
    "print('uploaded training data location: {}'.format(s3_train_data))\n",
    "\n",
    "output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "print('training artifacts will be uploaded to: {}'.format(output_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, create a docker container in the same region where our notebook is created. \n",
    "# In our case, it is \" us-east-1\"\n",
    "\n",
    "# containers = {\n",
    "#               'us-east-1': '382416733822.dkr.ecr.us-east-1.amazonaws.com/linear-learner:latest'\n",
    "#               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'linear-learner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# containers[boto3.Session().region_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the Logistic Regression Estimator\n",
    "\n",
    "# Now we begin to set up the Estimator. \n",
    "# SageMaker will not let you use any of their smaller (i.e. less expensive) images, \n",
    "# so here we use a virtual machine of size ml.p2.xlarge.\n",
    "\n",
    "#We set up the linear estimator\n",
    "linear = sagemaker.estimator.Estimator(container,\n",
    "                                       role=role, \n",
    "                                       train_instance_count=1, \n",
    "                                       train_instance_type='ml.c4.xlarge',\n",
    "                                       output_path=output_location,\n",
    "                                       sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Now we provide hyperparameters. There are many, like which loss function to use. \n",
    "# Here we put only the most important ones\n",
    "# feature_dim - It is the number of columns in our feature array. In this case it is 12.\n",
    "# mini_batch_size - It is the number of batches into which to split the data. \n",
    "#                   This number should be smaller than the number of records in our training set. \n",
    "#                   We only have 20000 records, so we take 2000 as batch size.\n",
    "# predictor_type—we use binary_classifier, which means logistic regression.\n",
    "\n",
    "linear.set_hyperparameters(feature_dim=12,\n",
    "                           mini_batch_size=2000,\n",
    "                           predictor_type='binary_classifier')\n",
    "\n",
    "# When you run the fit() method Amazon will kick off this job. This will take several minutes to run.\n",
    "\n",
    "linear.fit({'train': s3_train_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploying the Model\n",
    "\n",
    "\n",
    "# When the training model is done, deploy it to an endpoint. \n",
    "# Remember that Amazon is charging you money now. \n",
    "# So when you get done delete your endpoints unless you want to be charged.\n",
    "# Deploying the linear model\n",
    "\n",
    "linear_predictor = linear.deploy(initial_instance_count=1,\n",
    "                                 instance_type='ml.c4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "linear_predictor.content_type = 'text/csv'\n",
    "linear_predictor.serializer = csv_serializer\n",
    "linear_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an array for testing.\n",
    "b = np.array(prosper_final1[20000:21000]).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating the Dataset\n",
    "\n",
    "# Now we run the prediction. We take 1000 data points and run the prediction.  \n",
    "\n",
    "result = linear_predictor.predict(b[0:1000])\n",
    "\n",
    "# Printing some 100 predictions\n",
    "for i in range(100):\n",
    "    # print(i)\n",
    "      print(result['predictions'][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo 1\n",
    "# To find the accuracy of the model, \n",
    "#   1. Precision - Precision tries to find out what proportion of positive identifications was actually correct.\n",
    "#   2. Recall - Recall tries to find what proportion of actual positives was identified correctly.\n",
    "\n",
    "# 1. Precision formula,\n",
    "#     precision = TP/(TP+FP)\n",
    "\n",
    "# 2. Recall formula,\n",
    "#     recall = TP/(TP+FN)\n",
    "# where,\n",
    "# TP(True Positive): A true positive is an outcome where the model correctly predicts the positive class.\n",
    "# TN(True Negative): It is an outcome where the model correctly predicts the negative class.\n",
    "# FP(False Positive): It is an outcome where the model incorrectly predicts the positive class.\n",
    "# FN(False Negative): It is an outcome where the model incorrectly predicts the negative class.\n",
    "\n",
    "        \n",
    "# Correct prediction\n",
    "true_positive = 0\n",
    "true_negative = 0\n",
    "# Wrong prediction\n",
    "false_positive = 0\n",
    "false_negative = 0\n",
    "\n",
    "for i in range(500):\n",
    "    # print(i)\n",
    "    original = float(prosper_final1['LoanStatus'][20000+i])\n",
    "    predicted = result['predictions'][i]['predicted_label']\n",
    "# Here we are calculating the parameters needed to calculate precision and recall    \n",
    "    if original == 1.0 :\n",
    "        if original == predicted :\n",
    "            # print(str(original) +\" : \"+str(predicted))\n",
    "            true_positive+= 1\n",
    "        else :\n",
    "            false_negative+=1      \n",
    "    elif original == 0.0 :\n",
    "        if original == predicted :\n",
    "            true_negative+= 1\n",
    "        else :\n",
    "            false_positive+=1\n",
    "            \n",
    "    # print (original)        \n",
    "    # print (predicted)\n",
    "\n",
    "# print(result)\n",
    "# print(\"TP \"+str(true_positive) +\" : TN \"+str(true_negative))\n",
    "# print(\"FP \"+str(false_positive)+\" : FN \"+str(false_negative))\n",
    "\n",
    "precision = ??? # Apply precision formula here and uncomment below line\n",
    "# print(\"Precision \"+str(precision))\n",
    "\n",
    "# Output similar to below\n",
    "\n",
    "# Precision 0.9916546414285714"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo 2\n",
    "\n",
    "recall = ??? # Apply recall formula here and uncomment below line\n",
    "# print(\"Recall \"+str(recall))\n",
    "\n",
    "# Output similar to below\n",
    "\n",
    "# Recall 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
