# Video Catalog 


## Investigate Labs

[[[https://github.com/elephantscale/deep-learning-labs-solutions]{.underline}](https://github.com/elephantscale/deep-learning-labs-solutions)]{dir="ltr"}

[[[https://github.com/elephantscale/starweaver-labs]{.underline}](https://github.com/elephantscale/starweaver-labs)]{dir="ltr"}

[[[https://github.com/elephantscale/ml-labs-python-solutions]{.underline}](https://github.com/elephantscale/ml-labs-python-solutions)]{dir="ltr"}

[[[https://github.com/elephantscale/ml-labs-spark-python-solutions]{.underline}](https://github.com/elephantscale/ml-labs-spark-python-solutions)]{dir="ltr"}

[ ]{dir="ltr"}



## Machine Learning


1. Video for Git Repo (Getting Started with Machine Learninng)


[[[Link]{.underline}](https://drive.google.com/open?id=1E3TyJtZhLtfjDYcdfQWdXRGtZt-tfdah)]{dir="ltr"}

[]{dir="ltr"}

**[[Title:]{.underline}]{dir="ltr"}**

[]{dir="ltr"}

[Getting Started with Machine Learning: Cloning Git
Repository]{dir="ltr"}

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[]{dir="ltr"}

[Cloning the Git repository with the data for the Machine Learning
course. How to set up the data for the course. Using Github
Desktop]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[]{dir="ltr"}

[Minor editing required. Could need to be re-recorded if the data is no
longer in that repository.]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:05:40]{dir="ltr"}

[]{dir="ltr"}

2. V2-1-Lab-2a (Week 1 Brief Introduction to Pandas)

[[[Link]{.underline}](https://drive.google.com/open?id=1f-E7AncScFPZ7NwzzbbZEsdLAFudoz7p)]{dir="ltr"}

[]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Brief Introduction to Pandas]{dir="ltr"}

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Briefly introducing Lab 2a]{dir="ltr"}

[Open jupyter notebook, open pandas notebook, and go over pandas
notebook, have students do pandas lab]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[Nothing of note]{dir="ltr"}

[]{dir="ltr"}

**[[Related Lab:]{.underline}]{dir="ltr"}**

[W1-2a-pandas.ipynb]{dir="ltr"}

[Tested, no issues]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:01:52]{dir="ltr"}

[]{dir="ltr"}

 [V2-1-Pandas (Video 2: Introducing Datasets)]{dir="ltr"}
--------------------------------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1P306HqafzdvpYbPEYbxNYdkf46HTCFw6)]{dir="ltr"}

[]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Video 2: Introducing Datasets]{dir="ltr"}

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Introduction to Pandas, creating Pandas series and dataframe,
conducting various operations with a dataframe, descriptive statistics,
advanced pandas operations like concatenating, using real world datasets
and data exploration.]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[Nothing of note]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:35:26]{dir="ltr"}

[]{dir="ltr"}

 [V2-2-Exploring-Pandas(Pandas and Files)]{dir="ltr"}
----------------------------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1XGxKz5sxyQUrL30MGDiLP6Rsqpve0gFu)]{dir="ltr"}

[]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Pandas and Files]{dir="ltr"}

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Reading types of data files using pandas, saving data to files, dealing
with missing values, dealing with categorical variables. Finish with a
lab over NYCFlights13 dataset.]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[Nothing of note]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:09:10]{dir="ltr"}

 [V2-2-Lab-2b (Week 1 Lab 2b Exploring Pandas)]{dir="ltr"}
---------------------------------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1OBzza_i5InONHjVPlTHTWvuYMq2ka-6B)]{dir="ltr"}

[]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Exploring Pandas]{dir="ltr"}

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Going over pandas lab over the New York Dataset.]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[Nothing of note]{dir="ltr"}

[]{dir="ltr"}

**[[Related Lab:]{.underline}]{dir="ltr"}**

[W1-2b-exploring-pandas.ipynb]{dir="ltr"}

[Tested, no issues]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:02:07]{dir="ltr"}

 [V2-3-Visualization (Python: Visualizations)]{dir="ltr"}
--------------------------------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1ddNGfqLb1ewH1kXTPukqbJmVEkbQSQDk)]{dir="ltr"}

[]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Visualizations]{dir="ltr"}

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Learn about Python Visualization, look over different python
packages(matplotlib, seaborn, ggpot, etc..), and do comparison with
statistical graphs .]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[Nothing of note]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:18:51]{dir="ltr"}

 [V2-3-Lab-2c (Week 1 Lab 2c Intro Visualizations Lab)]{dir="ltr"}
-----------------------------------------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1scO6eoY0nnUHln-Sx8-fV_p4fIoaZl6P)]{dir="ltr"}

[]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Intro Visualizations Lab]{dir="ltr"}

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Going over the cars dataset using visualizations with matplotlib and
pandas. Brief look at the visualization lab. .]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[Nothing of note]{dir="ltr"}

[]{dir="ltr"}

**[[Related Lab:]{.underline}]{dir="ltr"}**

[W1-2c-visualization-cars.ipynb]{dir="ltr"}

[Tested, no issues]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:01:48]{dir="ltr"}

 [V2-4-Lab-2d (Week 1 Lab 2d: Visualization lab)]{dir="ltr"}
-----------------------------------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1USGuVG0WZYs5xob3CWVu7MSWowo3PUnQ)]{dir="ltr"}

[]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Week 1 Lab 2d: Visualization lab]{dir="ltr"}

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Going over to the visualization lab using matplotlib and seaborn. Brief
look over the lab.]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[Nothing of note]{dir="ltr"}

[]{dir="ltr"}

**[[Related Lab:]{.underline}]{dir="ltr"}**

[W1-2d-visualization-stats.ipynb]{dir="ltr"}

[Tested, no issues.]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:01:30]{dir="ltr"}

 [V2-4-Visualization-Stats (Data Exploration)]{dir="ltr"}
--------------------------------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1sbW9OuHh-sy9OixvMMKcJt0OsDBhmeJB)]{dir="ltr"}

[]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Data Exploration]{dir="ltr"}

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Going further with data exploration. Using numerical data analysis,
covariance/correlation, and visualizing. Learn to do explorative data
analysis and some statistics for data science. Going over data
types]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[Nothing of note]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:01:30]{dir="ltr"}

[]{dir="ltr"}

[V3-1-Lab-3a (Week 1 Lab 3a: Intro Scikit-learn)]{dir="ltr"}
------------------------------------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1nVDjvzEnimi6L_quWtG-E5KNmIGNrGyk)]{dir="ltr"}

[]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Week 1 Lab 3a: Intro Scikit-learn]{dir="ltr"}

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Intro to SciKit-learn lab. Brief overview of lab.]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[Nothing of note]{dir="ltr"}

[]{dir="ltr"}

**[[Related Lab:]{.underline}]{dir="ltr"}**

[W1-3a-sklearn-intro.ipynb]{dir="ltr"}

[Tested, updated code]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:01:30]{dir="ltr"}

 [V3-1-Sklearn (Video 3: Scikit-Learn Intro with Linear Regression)]{dir="ltr"}
------------------------------------------------------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1V4n6ih0t_rRTk9gm1yFuyRNGYxGFaNJM)]{dir="ltr"}

[]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Video 3: Scikit-Learn Intro with Linear Regression]{dir="ltr"}

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Understand Scikit-learn in python, advantages and disadvatanges,
supervised machine learning, unsupervised machine learning, and
recommendations.]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[Nothing of note]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:29:55]{dir="ltr"}

[]{dir="ltr"}

 [V3-2-Lab-3b (Week 1 Lab 3b: Linear Regression Lab)]{dir="ltr"}
---------------------------------------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1uMk4cIPxN8TQJSmS3i9a1K1vx4Atam6F)]{dir="ltr"}

[]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Week 1 Lab 3b: Linear Regression Lab: Linear Regression Lab]{dir="ltr"}

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Quick overview of scikit-learn lab of bills and tips lab, building a
linear model with data. Using linear regression in
Scikit-learn.]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[Nothing of note]{dir="ltr"}

[]{dir="ltr"}

**[[Related Lab:]{.underline}]{dir="ltr"}**

[W1-3b-sklearn-lr.ipynb]{dir="ltr"}

[Tested, no issues]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:01:38]{dir="ltr"}

 [V3-2-Linear Regression (Session: Linear Regression)]{dir="ltr"}
----------------------------------------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1gfylVx66XJNXqQwxk74Xw4-ECdngcBgt)]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Session: Linear Regression]{dir="ltr"}

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Understanding linear regression with one variable, introducing the
model, explaining gradient descent solution, and understanding linear
regression with many variables.]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[Nothing of note]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:12:50]{dir="ltr"}

[]{dir="ltr"}

 [V3-3-LogisticRegression (Many Variables:Multiple Linear Regression)]{dir="ltr"}
--------------------------------------------------------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=18ySotgC8qNRSc9p28lLZuE_4dUZ8AHz_)]{dir="ltr"}

[]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Session: Linear Regression]{dir="ltr"}

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Understanding multiple linear regression. One variable, gradient
descent, many variables. Solutions advice by verifying the dimensions,
feature scaling, and choosing the learning rate.]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[Titled wrong. Should be multiple linear regression.]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:07:16]{dir="ltr"}

[]{dir="ltr"}

[]{dir="ltr"}

 [W1\_V3-4-LogisticRegression (Logistic Regression)]{dir="ltr"}
--------------------------------------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1VrTDBaI-QO2BHKING70dOz1C2YL4sqo5)]{dir="ltr"}

[]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Logistic Regression]{dir="ltr"}

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Introduction to logistic regression and its applications. Includes an
overview of logistic regression to classification problems, the formula,
multiple logistic regression, how to prepare data for logistic
regression]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[The content is good but the sound quality is poor throughout most of
the video.]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:10:54]{dir="ltr"}

[]{dir="ltr"}

 [W2\_V1a-Classification\_SVM (SVM Classification)]{dir="ltr"}
-------------------------------------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1AKOSc9Z6BxhT-2BaWUh-QCgTP7Josn0n)]{dir="ltr"}

[]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Video 1: Classification using SVM]{dir="ltr"}

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Overview of Support Vector Machines (SVM) in Classification problems
including theory, maximal margin classifiers, hyper planes, soft margin
classifiers. The different kinds of kernels used with SVMs are also
covered in this video as well as how to prepare data for the SVM and
what the potential drawbacks are.]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[No edits required]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:21:19]{dir="ltr"}

[]{dir="ltr"}

[]{dir="ltr"}

 [W2\_V1\_Lab\_1a\_1b(SVM College Admission Lab)]{dir="ltr"}
-----------------------------------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1Ufiw-a4EibW3l225LK4NjjRReJlnGyZV)]{dir="ltr"}

[]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Session: SVM College Admission Lab]{dir="ltr"}

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Introduction to SVM lab which includes admissions data and then creates
a model to classify students as either admitted or not admitted. Builds
on the SVM lecture that precedes it.]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[No editing required.]{dir="ltr"}

[]{dir="ltr"}

**[[Related Lab:]{.underline}]{dir="ltr"}**

[W2-1a-svm-college.ipynb]{dir="ltr"}

[Tested and fixed]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:02:38]{dir="ltr"}

[]{dir="ltr"}

[]{dir="ltr"}

 [W2\_V1b\_Classification\_NB (ROC and Naive Bayes)]{dir="ltr"}
--------------------------------------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1wjZadBHa6IgxSV3AZ1SmvzAvlpnDT5A1)]{dir="ltr"}

[]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Video 1: Classification: ROC and Naive Bayes]{dir="ltr"}

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Further coverage of classification models and evaluation metrics with
the confusion matrix, precision, recall and the ROC curve. Then the
video gives an overview of Naive Bayes as a useful classification model
including an overview of the math behind it. Finally, an overview of how
to prepare data for Naive Bayes and the strengths and weaknesses of this
class of models.]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[No edits required]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:27:41]{dir="ltr"}

[]{dir="ltr"}

[]{dir="ltr"}

 [W2\_V2b\_Lab\_2c(Naive Bayes Lab)]{dir="ltr"}
----------------------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1K00-NwRAT6sDFUcjNSYGcfYr2PQsvcw_)]{dir="ltr"}

[]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Naive Bayes Census Lab]{dir="ltr"}

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Introduction to Naive Bayes classifier lab. Using the census dataset,
the goal is to create a classifier to determine if someone is making
above or below 50,000 a year. This lab uses the multinomial naive bayes
from sklearn to create this classifier.]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[No editing required.]{dir="ltr"}

[]{dir="ltr"}

**[[Related Lab:]{.underline}]{dir="ltr"}**

[W2-2c-nbayes-income.ipynb]{dir="ltr"}

[Tested, no issues]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:02:42]{dir="ltr"}

 [W2\_V2a\_Decision\_Trees (Decision Trees)]{dir="ltr"}
------------------------------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1m1Hqm6SYOxHyK4zjHc5UdhvuvT4t2SV6)]{dir="ltr"}

[]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Week 2 Video 2: Decision Trees]{dir="ltr"}

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Introduction to the decision tree model and eventually applying it to
the college admissions dataset introduced previously. Focus is on the
algorithm and use cases applied to different examples. More advanced
concepts such as pruning are also covered. Finally the strength and
weaknesses of decision trees are covered as well as the sklearn
implementation]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[No edits required]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:27:02]{dir="ltr"}

 [W2\_V2b\_Lab\_2a\_2b(Decision Tree Prosper Loan)]{dir="ltr"}
-------------------------------------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1aBwnGSBl8Zy3ZNGwiKcyZS4fAg1BhPTD)]{dir="ltr"}

[]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Decision Tree Prosper Loan Lab]{dir="ltr"}

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Introduction to Decision Tree lab. The goal is to use a single decision
tree from the sklearn implementation to determine the loan status of
individuals in the prosper loan dataset.]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[No editing required.]{dir="ltr"}

[]{dir="ltr"}

**[[Related Lab:]{.underline}]{dir="ltr"}**

[W2-2a-dec-tree-prosper.ipynb]{dir="ltr"}

[Tested and fixed]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:02:58]{dir="ltr"}

 [W2\_V2b\_RandomForests (Random Forests)]{dir="ltr"}
----------------------------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1_Bd9KOJw6jbXsrDN1LWBwMf5eS4-K0V6)]{dir="ltr"}

[]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Week 2 Video 2b: Random Forest]{dir="ltr"}

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[An overview of the problems with decision tree models, the
bias-variance tradeoff and how random forests are an evolution of the
decision and seek to mitigate some of these issues. It also introduces
the concept of model ensembling to create more powerful models. Data
bagging is presented as a method of preventing overfitting. There is a
discussion of the strengths and weaknesses of random forests and
introduces the sklearn implementation. The end of the video also
introduces the Random Forest lab which explains why there is not an
additional video for it.]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[No edits required]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:13:21]{dir="ltr"}

[]{dir="ltr"}

 [W2\_V3a\_Clustering (K-means Clustering)]{dir="ltr"}
-----------------------------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1la_vr4S0-vj9g_wHIklL9PK99XuGYmSF)]{dir="ltr"}

[]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Week 2 Video 3a: Unsupervised Learning: K-means Clustering]{dir="ltr"}

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[An introduction to unsupervised learning approaches and how they differ
from supervised learning. The video also covers some use cases for
unsupervised learning and introduces clustering. This includes an
introduction to the concepts, examples and algorithms. Then the video
moves onto k-means clustering as an overview of the concept and the
algorithm. There is then a focus on how to evaluate k-means performance
such as WSSE and then the complexity of the algorithm and some
drawbacks. Finally the video discusses strengths and weaknesses of the
method and the implementation in python using sklearn.]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[No edits required\
]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:26:36]{dir="ltr"}

[]{dir="ltr"}

 [W2\_V3a\_Lab\_3a\_3b(Kmeans mtcars)]{dir="ltr"}
------------------------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1-qlKzGQI5PxhJtxRbCgHcPge6IJKBXfb)]{dir="ltr"}

[]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Week 2 Lab 3a: Kmeans mtcars example]{dir="ltr"}

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Introduction to Kmeans clustering lab. The goal of this lab is to
calculate Kmeans clusters on the mtcars dataset using the sklearn
implementation of this algorithm. The lab also implements hyperparameter
tuning to use the elbow method to determine the best value for k by
calculating wsse.]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[No editing required.]{dir="ltr"}

[]{dir="ltr"}

**[[Related Lab:]{.underline}]{dir="ltr"}**

[W2-3a-kmeans-mtcars.ipynb]{dir="ltr"}

[Tested and fixed]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:03:52]{dir="ltr"}

 [W2\_V3b\_PCA (PCA)]{dir="ltr"}
-------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1vFp6sWfNtlhe5kI4qrjskOKLSkr41yas)]{dir="ltr"}

[]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Week 2 Video 3b: Unsupervised Learning: PCA]{dir="ltr"}

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[An introduction to dimensionality reduction as a method of unsupervised
learning. The first part of the video covers why dimensionality
reduction is effective. Specifically, the video covers Principle
Component Analysis (PCA) as well as why this is different than
clustering.This includes the concept as well as an example with
calculation behind the method. Additionally, the video covers
evaluation, visualizing of PCA and use cases and the sklearn
implementation.]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[No edits required]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:20:59]{dir="ltr"}

[]{dir="ltr"}

 [W2\_V3b\_PCA\_Lab\_3c ( PCA Lab)]{dir="ltr"}
---------------------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1-qlKzGQI5PxhJtxRbCgHcPge6IJKBXfb)]{dir="ltr"}

[]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Week 2 Lab 3c: PCA Wine Quality Example]{dir="ltr"}

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Introduction to PCA lab. The goal is to use PCA to reduce the
dimensionality on the wine quality dataset. This is done using the
sklearn implementation and using a biplot.]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[No editing required as long as code still exists.]{dir="ltr"}

[]{dir="ltr"}

**[[Related Lab:]{.underline}]{dir="ltr"}**

[w2-3c-pca-wine.ipynb]{dir="ltr"}

[Tested, no issues]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:04:21]{dir="ltr"}

 [W3\_V1a\_Lab\_1a (Week 3 Lab 1a Deep Learning Playground)]{dir="ltr"}
----------------------------------------------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1EGzdgExrZf-kTxtIz_I4M4UFhqrBrA4N)]{dir="ltr"}

**[[Title:]{.underline}]{dir="ltr"}**

[Deep Learning Playground]{dir="ltr"}
-------------------------------------

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Brief introduction to the Tensorflow Playground.
[[https://playground.tensorflow.org/]{.underline}](https://playground.tensorflow.org/).
How to manipulate the program to visualize deep learning training and
model manipulation.]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[]{dir="ltr"}

[Lab has been checked and is good to go.]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:03:20]{dir="ltr"}

[]{dir="ltr"}

 [W3\_V1b\_Lab\_1b (Week 3 Lab 1b Understanding Tensorflow Sessions)]{dir="ltr"}
-------------------------------------------------------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1opaLMW1f6kR3PyG5kDOF4rZ6vi_eDlqh)]{dir="ltr"}

**[[Title:]{.underline}]{dir="ltr"}**

[Understanding Tensorflow Sessions]{dir="ltr"}
----------------------------------------------

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Introduction to Tensorflow Sessions lab. Lazy evaluation and graph
output. Eager evaluation in Tensorflow.]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[]{dir="ltr"}

[No editing needed as long as related lab still exists in
github]{dir="ltr"}

[]{dir="ltr"}

**[[Related Lab:]{.underline}]{dir="ltr"}**

[W3-1b-session.ipynb]{dir="ltr"}

[Lab is fixed to tf version 2.0]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:01:24]{dir="ltr"}

[]{dir="ltr"}

 [W3\_V2a\_Lab\_2a\_2b (Week 3 Lab 2a: Tips and 2b: MNIST Linear)]{dir="ltr"}
----------------------------------------------------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1m-phgVmSeKqgljJ0frxkjeZ0on7giHZT)]{dir="ltr"}

**[[Title:]{.underline}]{dir="ltr"}**

[Linear Regression in Tensorflow]{dir="ltr"}
--------------------------------------------

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Introduction to Tensorflow Regression labs: bill vs. tip example and
linear MNIST. Set up a class for tf linear regression and plotting
results. Use of tensorflow variables, gradient descent as applied to low
level API use of tensorflow.]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[]{dir="ltr"}

[No editing needed as long as related labs still exists in
github]{dir="ltr"}

[Code must be updated to tf v2.0]{dir="ltr"}

[]{dir="ltr"}

**[[Related Labs:]{.underline}]{dir="ltr"}**

[w3-2a-tips.ipynb : Fixed to tf v2.0]{dir="ltr"}

[w3-2b-mnist-linear.ipynb]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:02:22]{dir="ltr"}

[]{dir="ltr"}

 [W3\_V2b\_Lab\_2c\_2d (Week 3 Lab 2c: Estimator Cars and 2d: Keras Linear MNIST)]{dir="ltr"}
--------------------------------------------------------------------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1bc3IVBsovm0NmvFRljvIaLrMmti9L4x_)]{dir="ltr"}

**[[Title:]{.underline}]{dir="ltr"}**

[Keras Estimator API in TensorFlow]{dir="ltr"}
----------------------------------------------

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Introduction to Tensorflow Keras Estimator labs: cars and linear MNIST.
Building a linear regression estimator and run predictions. Use of Keras
API to build estimator. Simple model construction for linear
regression.]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[]{dir="ltr"}

[No editing needed as long as related labs still exists in
github]{dir="ltr"}

[Code must be updated to tf v2.0]{dir="ltr"}

[]{dir="ltr"}

**[[Related Labs:]{.underline}]{dir="ltr"}**

[w3-2c-estimator-cars.ipynb]{dir="ltr"}

[W3-2d-keras-linear-mnist.ipynb : Lab tested and ready to go]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:02:15]{dir="ltr"}

[]{dir="ltr"}

 [W3\_V3a\_MLP (Week 3 video: Multi-Layer Perceptrons)]{dir="ltr"}
-----------------------------------------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1ToDUx8NrzJVvT0tjfhoyzVHSowZKUr9R)]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Mult-Layer Perceptrons]{dir="ltr"}
-----------------------------------

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[An introduction to Multi-Layer Perceptrons including Hidden Layers,
optimizers, and activation functions. An overview on the basics of deep
learning optimizers including gradient descent, momentum, Adam.
Activation functions covered include sigmoid and tanh activation
functions, the vanish gradient concept, softmax.]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[]{dir="ltr"}

[No editing required]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:21:45]{dir="ltr"}

[]{dir="ltr"}

[]{dir="ltr"}

[]{dir="ltr"}

[]{dir="ltr"}

 [W3\_V3a\_Lab\_3a (Week 3 Lab 3a: DL playground with hidden layer)]{dir="ltr"}
------------------------------------------------------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1ToDUx8NrzJVvT0tjfhoyzVHSowZKUr9R)]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Deep Learning Playground with Hidden Layers]{dir="ltr"}
--------------------------------------------------------

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Use of Tensorflow Playground to create a simple Multi-Layer Perceptron
(MLP).
[[https://playground.tensorflow.org/]{.underline}](https://playground.tensorflow.org/).
Adding hidden layers to the network in the program to see the effect
when changing the amount of neurons and applying to different standard
datasets]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[]{dir="ltr"}

[Edit to change the abrupt ending. Check to see if lab still
exists]{dir="ltr"}

[]{dir="ltr"}

**[[Related Labs:]{.underline}]{dir="ltr"}**

[W3-3a-dl-playground-hidden-layer.md : Checked and Good to
go.]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:01:51]{dir="ltr"}

[]{dir="ltr"}

 [W3\_V3a\_Lab\_3b\_3c (Week 3 Lab 3b: DNN Low Level Intro and Lab 3c: )]{dir="ltr"}
-----------------------------------------------------------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1kIszdMG8RcQNGaUgYJXDkkauSYUr6uma)]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[DNN Intro and DNN Intro using Keras]{dir="ltr"}
------------------------------------------------

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Introduction to Lab 3b and Lab 3c. Creation of a DNN using the low
level tensorflow API to create an estimator. Creation of a DNN using the
keras implementation of DNN to streamline model construction on the
MNIST dataset.]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[]{dir="ltr"}

[]{dir="ltr"}

[Code must be updated to tf v2.0]{dir="ltr"}

[]{dir="ltr"}

**[[Related Labs:]{.underline}]{dir="ltr"}**

[W3-3b-dnn-intro-lowlevel.ipynb : Lab edited to tf v2.0]{dir="ltr"}

[W3-3c-dnn-intro-keras.ipynb]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:02:57]{dir="ltr"}

 [W3\_V3b\_Lab\_3d\_3e (Week 3 Lab 3d: DNN Iris Estimator and Lab 3e: DNN Iris using Keras )]{dir="ltr"}
-------------------------------------------------------------------------------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1OtvQ9wBsCkWPapSq-DQ8XBW0yXyKtcdz)]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[DNN Iris Classifier Intro and DNN Iris Classifier Intro using Keras]{dir="ltr"}
--------------------------------------------------------------------------------

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Introduction to Lab 3d and Lab 3e. Using a simple DNN to create a
classifier on the Iris dataset with tensorflow. The first lab is using
the estimator API and the second is using the Keras API]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[]{dir="ltr"}

[]{dir="ltr"}

[Code must be updated to tf v2.0]{dir="ltr"}

[]{dir="ltr"}

**[[Related Labs:]{.underline}]{dir="ltr"}**

[w3-3d-dnn-estimator-irisipynb]{dir="ltr"}

[W3-3e-dnn-keras-iris.ipynb]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:04:32]{dir="ltr"}

[]{dir="ltr"}

[]{dir="ltr"}

[W4\_V01\_CNN (CNN)]{dir="ltr"}
-------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=19s_M_VxngyUU0Ua_K3IkRj_1vZz1Rl75)]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Introducing CNNs]{dir="ltr"}
-----------------------------

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[An introduction to image recognition and why CNNs help mitigate some of
the problems with feed forward networks on large amounts of image data.
Next is an overview of the history of CNNs and the important concepts
such as convolutions, pooling, ReLU activation. Then there is a full
example of the computation of convolution on an example. Finally, a walk
through of the full architecture of a CNN and major issues.]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[]{dir="ltr"}

[No editing required]{dir="ltr"}

[]{dir="ltr"}

[]{dir="ltr"}

[]{dir="ltr"}

**[[Related Labs:]{.underline}]{dir="ltr"}**

[None]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:33:30]{dir="ltr"}

[W4\_V02\_CNN (CNN in Tensorflow)]{dir="ltr"}
---------------------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1Zd6Bml5NHctBuU-fyiRigjiHk9Nmwnj3)]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[CNN in Tensorflow]{dir="ltr"}
------------------------------

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[The video is primarily about implementing CNNs in Tensorflow and
includes information such as what the input tensor shapes should be and
how to create the convolutional layer and the pooling layer. The
convolutions intro lab is shown as a code example. Next the CNN MNIST
lab is introduced and the code is shown which has implementations in
both base tf and keras. Finally, the CNN fashion lab is introduced and
the code is shown.]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[]{dir="ltr"}

[All code shown in both the slides and the labs is from tensorflow v1
and must be changed to the new version.]{dir="ltr"}

[]{dir="ltr"}

[]{dir="ltr"}

[]{dir="ltr"}

**[[Related Labs:]{.underline}]{dir="ltr"}**

[6.1-convolutions-intro.ipynb]{dir="ltr"}

[6.2-cnn-mnist.ipynb]{dir="ltr"}

[6.3-cnn-fashion.ipynb]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:20:45]{dir="ltr"}

[]{dir="ltr"}

[W4\_V03\_Tensorboard (Tensorboard)]{dir="ltr"}
-----------------------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1_iFYZku_5bEZQYoP3S4wU5tTsbox2rTU)]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Tensorboard]{dir="ltr"}
------------------------

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[]{dir="ltr"}

[The video introduces the tensorboard tool from tensorflow and shows an
example with MNIST in both jupyter notebook and colab. The focus is on
just running a tensorflow model and then visualizing the results using
tensorboard.]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[]{dir="ltr"}

[The code shown from tensorflow in the lab needs to be updated to tf
v2.0.]{dir="ltr"}

[]{dir="ltr"}

[Video can be edited around 6:50 to 7:30 to remove time waiting for code
to run. Video can also be edited from 8:40 to 9:50 to remove more dead
air.]{dir="ltr"}

[]{dir="ltr"}

[]{dir="ltr"}

[]{dir="ltr"}

**[[Related Labs:]{.underline}]{dir="ltr"}**

[w4-4-tensorboard.ipynb]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:10:33]{dir="ltr"}

[]{dir="ltr"}

 [W5\_V1\_Transferlearning (Transfer Learning)]{dir="ltr"}
---------------------------------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=14YIPkOFO-ljYpND2r73na49IK7rNmraC)]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Transfer Learning]{dir="ltr"}
------------------------------

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Understanding transfer learning, pre-trained models, and customizing
pretrained models. Look at some examples of transfer learning models.
Applying the model to your data. Comparison of different transfer
learning architectures. Inception is best on performance, accuracy, and
size of network.]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[]{dir="ltr"}

[Slide title is misspelled, "Transfer Leraning. Tim starts yawning
around 5:25 and 6:05 and 14:30]{dir="ltr"}

[]{dir="ltr"}

**[[Related Labs:]{.underline}]{dir="ltr"}**

[None]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:15:51]{dir="ltr"}

 [W5\_V2\_RNN (Tensorflow RNN)]{dir="ltr"}
-----------------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1CMPq5xStqNnNb1KMfT6DTv1w_Dsgndrf)]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Introducing RNN's]{dir="ltr"}
------------------------------

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Understanding the meaning of Recurrence and RNN's, why recurrence
creates memory and state in NN, and learn how to implement RNN in
Tensorflow. Different types of RNN's. Disadvantages of RNN's. Show lab
doing a manual RNN, using a static RNN, packing sequences, dynamic RNN,
and multi-layer RNN.]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[]{dir="ltr"}

[No notes]{dir="ltr"}

[]{dir="ltr"}

**[[Related Labs:]{.underline}]{dir="ltr"}**

[]{dir="ltr"}

[W5-2a-rnn-intro.ipynb]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:29:30]{dir="ltr"}

 [W5\_V3\_LSTM (Long Short Term Memory (LSTM) Neural Networks)]{dir="ltr"}
-------------------------------------------------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1CMPq5xStqNnNb1KMfT6DTv1w_Dsgndrf)]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Long Short Term Memory (LSTM) Neural Neworks]{dir="ltr"}
---------------------------------------------------------

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Learn about Long Short Term Memory(LSTM) Neural networks and understand
how to use them. One of the issues with RNN are long training times.
Components of LSTM, architecture, testing LSTM models. Use LSTM model on
MNIST dataset. Show lstm with stocks data. LSTM and NLP. LSTM using
word2vec using a txt data set.]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[LSTM-stocks notebook had issues with loading stocks data during
video.]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:34:50]{dir="ltr"}

[]{dir="ltr"}

**[[Related Labs:]{.underline}]{dir="ltr"}**

[]{dir="ltr"}

[8.2-lstm-intro.ipynb]{dir="ltr"}

[8.3-lstm-stocks.ipynb]{dir="ltr"}

[8.4-word2vec.ipynb]{dir="ltr"}

 [W6\_V1\_Distributed (Week 6 Video: Scaling Machine Learning )]{dir="ltr"}
--------------------------------------------------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1OtvQ9wBsCkWPapSq-DQ8XBW0yXyKtcdz)]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Scaling Machine Learning]{dir="ltr"}
-------------------------------------

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Overview of scaling ML including distributed TensorFlow, Container
Cluster Orchestration, Big Data Ecosystem, Cloud Platforms. Tensorflow
is optimized for use with GPUs and can be parallelized and scaled for
distributed training. Model serving cluster is useful in scaling across
multiple nodes. There is also an example using distributed tensorflow
that is presented. Container clusters such as Kubernetes can be useful
for scaling. Spark is another framework that allows for scalability in
machine learning. Finally this video covers the cloud platforms such as
AWS(Amazon) and GCP(Google) which both have tools for machine learning
on their platform. The Pancake stack as a solution for deep learning on
Big Data is also introduced.]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[]{dir="ltr"}

[]{dir="ltr"}

[Possible update of code to tf v2.0. No other edits required but sound
quality does seem to fluctuate.]{dir="ltr"}

[]{dir="ltr"}

**[[Related Labs:]{.underline}]{dir="ltr"}**

[9.1-distributed.ipynb]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:31:30]{dir="ltr"}

[]{dir="ltr"}

 [W6\_V2\_FeatureEng (Week 6 Video: Feature Engineering)]{dir="ltr"}
-------------------------------------------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1ob1poVh1Xw_OoamDvtTUhKniNKGmBpDC)]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Feature Engineering]{dir="ltr"}
--------------------------------

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Overview of Feature Engineering and how to derive features from raw
data. This is done by mathematical transformation or mapping to an
output numerical feature that we can use more effectively in our models.
This video also contains an introduction to Kaggle and the Titanic
dataset. Transforming data columns into new features can also lend a lot
more power to a model. Feature Engineering is often done using domain
knowledge, understanding of data and experience. The end introduces a
lab listed below that uses the feature tools package and nyc taxi
data]{dir="ltr"}

[]{dir="ltr"}

**[[Edit Notes :]{.underline}]{dir="ltr"}**

[]{dir="ltr"}

[]{dir="ltr"}

[Make sure that lab still exists]{dir="ltr"}

[]{dir="ltr"}

**[[Related Labs:]{.underline}]{dir="ltr"}**

[w6-2-featuretools-nyc.ipynb]{dir="ltr"}

[w6-2-featuretools-uk.ipynb]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:25:19]{dir="ltr"}

**[Big Data]{dir="ltr"}**
=========================

 [Big Data Overview]{dir="ltr"}
------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1Dj4hiedhn5beN8z-V0Qrd1YkDYN6NYMb)]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Big Data Overview]{dir="ltr"}
------------------------------

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Introduction video to a series on Big Data. It begins with an
introduction on what big data is and some use cases. The video then
switches to an overview of the challenges of working with big data and
performing analytics. It then presents a use case of building a data
platform to support connected devices and process data as it comes in.
It then begins coverage of the components of infrastructure: Capture,
Process, Visualization, Store and Analytics. The video ends after
covering capture and processing.]{dir="ltr"}

[]{dir="ltr"}

[**[Edit Notes :]{.underline}**]{dir="ltr"}

[No edits required]{dir="ltr"}

[]{dir="ltr"}

**[[Related Labs:]{.underline}]{dir="ltr"}**

[None]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:49:29]{dir="ltr"}

 [Big Data Overview 2]{dir="ltr"}
--------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1RkmMZBkmssEdUf-_4_i9I6XhwntiJb7d)]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Big Data Overview 2]{dir="ltr"}
--------------------------------

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[A direct continuation of the previous video using the same slide deck.
It continues with discussion of processing then moves on to storage
including what specific technologies are used for each of these. Next,
Hadoop is introduced and covered in detail. NoSQL and CAP is covered.
Then analytics and access of the data is covered. Then the visualization
portion of the pipeline is presented. Finally, the whole pipeline is
covered and how it all comes together.]{dir="ltr"}

[]{dir="ltr"}

[**[Edit Notes :]{.underline}**]{dir="ltr"}

[No edits required]{dir="ltr"}

[]{dir="ltr"}

**[[Related Labs:]{.underline}]{dir="ltr"}**

[None]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:57:40]{dir="ltr"}

[]{dir="ltr"}

 [Big Data Ecosystem 4]{dir="ltr"}
---------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=1-GBscvqQr8Y5hsrPRLPeeVqRdheDbOds)]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Big Data Ecosystem 4]{dir="ltr"}
---------------------------------

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Fourth in a series about the Big Data Ecosystem. This video
specifically covers NoSQL and technologies such as Cassandra as a data
storage solution. It also contains a theoretical overview of the CAP
theorem of databases and use cases of Cassandra.]{dir="ltr"}

[]{dir="ltr"}

[**[Edit Notes :]{.underline}**]{dir="ltr"}

[Fourth in a series about the Big Data Ecosystem. It seems we are
missing the video before this one which would have been a deep dive
about Hadoop. This video individually does not require editing apart
from maybe removing references to the larger series.]{dir="ltr"}

[]{dir="ltr"}

**[[Related Labs:]{.underline}]{dir="ltr"}**

[None]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:26:51]{dir="ltr"}

 [Big Data Ecosystem 5]{dir="ltr"}
---------------------------------

[[[Link]{.underline}](https://drive.google.com/open?id=17ibSkFJd789_f7T4b-FtIS0rzRvMQWJT)]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Big Data Ecosystem 5]{dir="ltr"}
---------------------------------

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Fifth in a series about the Big Data Ecosystem. This video specifically
covers analytics in a big data ecosystem including traditional methods
as well as scalable methods. Next the video moves onto visualization in
the big data ecosystem including the commercial BI tools and open source
languages. Then the video discusses lambda implementation. Finally, the
video covers how all of the pieces of the ecosystem come
together.]{dir="ltr"}

[]{dir="ltr"}

[**[Edit Notes :]{.underline}**]{dir="ltr"}

[Fifth in a series about the Big Data Ecosystem. It seems we are missing
the earlier videos in this series. This video individually does not
require editing apart from maybe removing references to the larger
series.]{dir="ltr"}

[]{dir="ltr"}

**[[Related Labs:]{.underline}]{dir="ltr"}**

[None]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:26:00]{dir="ltr"}

**[ML with Spark]{dir="ltr"}**
==============================

[Exported Video Shortened]{dir="ltr"}
=====================================

[[[Link]{.underline}](https://drive.google.com/open?id=11eovcOG4Si10_ZA8XtTyy8SsLLbxYAe1)]{dir="ltr"}

[]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Scalability and PySpark for Data Science]{dir="ltr"}
-----------------------------------------------------

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[An introduction to scaling python and sklearn using Spark. This also
includes an example of PySpark using the wordcount example.]{dir="ltr"}

[]{dir="ltr"}

[**[Edit Notes :]{.underline}**]{dir="ltr"}

[Minor sound quality issues. Video is fairly short at 7
minutes.]{dir="ltr"}

[]{dir="ltr"}

**[[Related Labs:]{.underline}]{dir="ltr"}**

[None]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:07:37]{dir="ltr"}

[Spark 2.0 Machine Learning]{dir="ltr"}
=======================================

[[[Link]{.underline}](https://drive.google.com/open?id=0B3N2hPHCWPXYU2lvelBLTFR4ckU)]{dir="ltr"}

[]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Spark 2.0: RDD - API versus Dataframe API]{dir="ltr"}
------------------------------------------------------

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[A detailed overview of the code used in the different APIs used in
spark ml including VectorAssembler and MILib. This also uses a code
example presented in a lab that utilizes the ml tools in
pyspark.]{dir="ltr"}

[]{dir="ltr"}

[**[Edit Notes :]{.underline}**]{dir="ltr"}

[Large watermark from using unregistered copy of Screenflick]{dir="ltr"}

[]{dir="ltr"}

**[[Related Labs:]{.underline}]{dir="ltr"}**

[kmeans\_mtcars]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:11:50]{dir="ltr"}

**[Big DL]{dir="ltr"}**
=======================

1.  [Big DL Introduction]{dir="ltr"}

[[[Link]{.underline}](https://www.youtube.com/watch?v=vGfiwOIxVf8&t=865s)]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Big DL Introduction - Getting Started]{dir="ltr"}
--------------------------------------------------

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Introduction video to BigDL. Starts with Deep Learning History.
Discusses what is BigDL, a distributed deep learning library for Apache
Spark. Compare BigDL versus Tensorflow. Goes over GPUs and CPU. The
video ends after covering capture and processing. BigDL fills the gap in
Big Data with Deep Learning. Goes into how to run BigDL on your computer
using aws, sandbox, docker, etc. Show's setup of docker to use Big DL.
Accessing Big DL github repository. Using Elephant scale
[[sandbox]{.underline}](https://elephantscale.com/sandbox) and how to
set it up. Finishes with testing the notebook environment to see BigDL
is working.]{dir="ltr"}

[]{dir="ltr"}

[**[Edit Notes :]{.underline}**]{dir="ltr"}

[No edits required]{dir="ltr"}

[]{dir="ltr"}

[**[Related Labs:]{.underline}**]{dir="ltr"}

[[[Big DL Trainings
repo]{.underline}](https://github.com/intel-analytics/BigDL-trainings)]{dir="ltr"}

[Testing-123.ipynb]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:29:17]{dir="ltr"}

[]{dir="ltr"}

2.  [Big DL Creating a Basic Neural Network]{dir="ltr"}

[[[Link]{.underline}](https://www.youtube.com/watch?v=cdIwzxaZkwY&t=1s)]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Big DL Creating a Basic Neural Network]{dir="ltr"}
---------------------------------------------------

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Starts with Feed forward Neural Network on the Iris Dataset notebook.
Explains a simple neural network and the different layers. Going over
data and loading the data. Examining the training and validation class
distribution. Setting the network, training the model, and visualizing
results with tensorboard and a confusion matrix. See how well model
predicts and optimization methods.]{dir="ltr"}

[]{dir="ltr"}

[**[Edit Notes :]{.underline}**]{dir="ltr"}

[No edits required]{dir="ltr"}

[]{dir="ltr"}

[**[Related Labs:]{.underline}**]{dir="ltr"}

[[[Big DL Trainings
repo]{.underline}](https://github.com/intel-analytics/BigDL-trainings)]{dir="ltr"}

[Feedforward-iris.ipynb]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:21:26]{dir="ltr"}

[]{dir="ltr"}

3.  [Transfer Learning with BigDL]{dir="ltr"}

[[[Link]{.underline}](https://www.youtube.com/watch?v=m2mtc_YibFM)]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Transfer Learning for Image Classification in BigDL.]{dir="ltr"} 
------------------------------------------------------------------

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Going over Transfer Learning Architecture from imagenet model. Setup
AWS to use an Ubuntu instance to run the model. Use ubuntu instance to
run docker file. Running inceptions python script dealing with flowers
data set. Flowers have 5 labels. Run the imagenet pretrained model on
this data set. Using googlenet caffemodel and googlenet prototxt model
to import into model. Get results using the model.]{dir="ltr"}

[]{dir="ltr"}

[**[Edit Notes :]{.underline}**]{dir="ltr"}

[Process may take time as takes a bit to start up and to get results
from the model.]{dir="ltr"}

[]{dir="ltr"}

[**[Related Labs:]{.underline}**]{dir="ltr"}

[[[Big DL Trainings
repo]{.underline}](https://github.com/intel-analytics/BigDL-trainings)]{dir="ltr"}

[Inceptions\_transfer.py]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:14:39]{dir="ltr"}

[]{dir="ltr"}

4.  [Long Short Term Memory (LSTM) and Recurrent Neural Network(RNN) on
    > BigDL]{dir="ltr"}

[[[Link]{.underline}](https://www.youtube.com/watch?v=1NGsdjhds2w)]{dir="ltr"}

[**[Title:]{.underline}**]{dir="ltr"}

[Long Short Term Memory(LSTM) and Recurrent Neural Network(RNN) in BigDL]{dir="ltr"}
------------------------------------------------------------------------------------

[]{dir="ltr"}

**[[Description:]{.underline}]{dir="ltr"}**

[Starts going over the first notebook LSTM example of 20 newsgroup
classification. Show scraped data from 20 different newsgroups. Show's
LSTM model and goes over the parameters. Go over our optimizer to train
the model. Then go over to the next notebook looking at the Stock
Market. Seeing how well the model predicts the stock market. Goes over
similar steps to NLP model with the stock market model. Prediction
results were shown. Ends with summary of Big DL.]{dir="ltr"}

[]{dir="ltr"}

[**[Edit Notes :]{.underline}**]{dir="ltr"}

[Models were trained and there is a cut to fast forward through training
that may take longer to run in real time.]{dir="ltr"}

[]{dir="ltr"}

**[[Related Labs:]{.underline}]{dir="ltr"}**

[[**[BigDL-Training
repository]{.underline}**](https://github.com/intel-analytics/BigDL-trainings)]{dir="ltr"}

[lstm-20news.ipynb]{dir="ltr"}

[Lstm-stocks.ipynb]{dir="ltr"}

[]{dir="ltr"}

[**[Length :]{.underline}**]{dir="ltr"}

[00:19:58]{dir="ltr"}

[]{dir="ltr"}

[]{dir="ltr"}

[]{dir="ltr"}

[]{dir="ltr"}
